{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35684a3-858f-4274-b671-32f68cf41a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\samar\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samar\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\samar\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1755df60-0ea5-46f9-8a89-5103bb097ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\samar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34e749af-f3cf-4242-80e1-890cc9dc83ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome, to my repo of NLP check's.\n",
    "Please check out the ml repo too! with NLP check.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f126496e-f11a-4275-869d-d9b8e0204c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to my repo of NLP check's.\n",
      "Please check out the ml repo too! with NLP check.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55ddf2ad-36c7-4174-8167-32b556c20713",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenization\n",
    "###Paragraph ->sentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    "doc = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3f33587-b00a-4fcc-b0cf-47f0c767a9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Welcome, to my repo of NLP check's.\",\n",
       " 'Please check out the ml repo too!',\n",
       " 'with NLP check.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2822e351-6f24-4bdd-abe1-201c21d3f27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92304678-bd5b-432e-b01b-e5ff44dc38ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to my repo of NLP check's.\n",
      "Please check out the ml repo too!\n",
      "with NLP check.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcfdb43a-2bcc-4926-9b57-9a3504a3955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Sentence->word no not happening as its in list format\n",
    "###Para ->word\n",
    "from nltk.tokenize import word_tokenize\n",
    "word = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfb766c0-dd46-46b8-94a5-eede29b4947c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'my',\n",
       " 'repo',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'check',\n",
       " \"'s\",\n",
       " '.',\n",
       " 'Please',\n",
       " 'check',\n",
       " 'out',\n",
       " 'the',\n",
       " 'ml',\n",
       " 'repo',\n",
       " 'too',\n",
       " '!',\n",
       " 'with',\n",
       " 'NLP',\n",
       " 'check',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8af463c-72db-4d9e-9ae9-d83d1bd99070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'my',\n",
       " 'repo',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'check',\n",
       " \"'\",\n",
       " 's',\n",
       " '.',\n",
       " 'Please',\n",
       " 'check',\n",
       " 'out',\n",
       " 'the',\n",
       " 'ml',\n",
       " 'repo',\n",
       " 'too',\n",
       " '!',\n",
       " 'with',\n",
       " 'NLP',\n",
       " 'check',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###diiffrent with more power on symbol like ' \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ebd59-a28c-456b-85dd-1eab53f3a01c",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is a text normalization technique used in Natural Language Processing (NLP) that reduces words to their base or root form, known as the \"stem.\" The primary goal is to treat different variations of a word as a single item, which helps in reducing the dimensionality of the text data. For instance, the words \"connecting,\" \"connected,\" \"connection,\" and \"connects\" are all reduced to the stem \"connect.\"\n",
    "\n",
    "It's important to note that the resulting stem is not always a valid dictionary word. For example, the stem for \"studies\" and \"studying\" might be \"studi,\" which is not a real word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8407b139-a960-4ec9-a9bb-2ab7ddf1b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['eating','eaten','eats','writing','written','writes','programming','programs','trouble', 'troubled', 'troubling','history']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36fac2b-c043-4ff0-97df-771526b26464",
   "metadata": {},
   "source": [
    "## PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0a633a0-5e17-43ad-ba2d-03794c322f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab36bf75-1b69-43a1-a498-6ea66eac4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dd9896f-bb09-4e46-ab7e-0f666c5e2a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eaten--->eaten\n",
      "eats--->eat\n",
      "writing--->write\n",
      "written--->written\n",
      "writes--->write\n",
      "programming--->program\n",
      "programs--->program\n",
      "trouble--->troubl\n",
      "troubled--->troubl\n",
      "troubling--->troubl\n",
      "history--->histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"--->\"+stems.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "789dcb8f-16e6-4cbc-b22a-8ae05a2baefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems.stem(\"Sitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b838d8c-034c-479a-8ebc-19609ad2879a",
   "metadata": {},
   "source": [
    "## RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90c028de-0133-4ada-8049-9d21e4eea386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import  RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f020a611-9946-43a7-866b-344913f57f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b2d760d-7d97-48d7-bba9-0c4a972a8b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eaten--->eaten\n",
      "eats--->eat\n",
      "writing--->writ\n",
      "written--->written\n",
      "writes--->write\n",
      "programming--->programm\n",
      "programs--->program\n",
      "trouble--->troubl\n",
      "troubled--->troubled\n",
      "troubling--->troubl\n",
      "history--->history\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'--->'+reg.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6acdeaee-30fe-47bd-b974-684f1519fb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sitt'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.stem('Sitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a3b078-453f-4d1d-a545-1a66aa098670",
   "metadata": {},
   "source": [
    "## SnowBall Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf2d7118-14fb-4031-b662-762549593081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a032d79-f007-4426-9370-485a62e13a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8baebbb3-8b48-45f5-befc-657d1a72646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eaten--->eaten\n",
      "eats--->eat\n",
      "writing--->write\n",
      "written--->written\n",
      "writes--->write\n",
      "programming--->program\n",
      "programs--->program\n",
      "trouble--->troubl\n",
      "troubled--->troubl\n",
      "troubling--->troubl\n",
      "history--->histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'--->'+snow.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c48f18e-6073-425b-a481-663ac87c9b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'spotingli')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems.stem('fairly'),stems.stem('spotingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a7e733d-1d87-4af8-91b6-18ad8abf0fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'spotingly')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.stem('fairly'),reg.stem('spotingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "779276e6-211e-4c17-890a-e3199a8c7a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'spote')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow.stem('fairly'),snow.stem('spotingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f7fe2-0be0-40d9-badf-f4f9eaca9bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
